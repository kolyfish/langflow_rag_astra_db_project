#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®‰å…¨çš„ Astra DB è¨­ç½®è…³æœ¬
ä½¿ç”¨ç’°å¢ƒè®Šæ•¸æˆ–é…ç½®æ–‡ä»¶ä¾†å®‰å…¨å­˜å„² API Token
"""

import os
import json
import asyncio
from pathlib import Path
from astrapy import DataAPIClient
from openai import OpenAI

# æ‚¨çš„é…ç½®
ASTRA_DB_ID = "ef4581e5-f997-44ce-8432-e56636786548"
CHATGPT_API_KEY = os.getenv("OPENAI_API_KEY", "YOUR_OPENAI_API_KEY_HERE")

def get_astra_token():
    """å®‰å…¨ç²å– Astra DB Token"""
    # æ–¹æ³•1: å¾ç’°å¢ƒè®Šæ•¸ç²å–
    token = os.getenv('ASTRA_DB_TOKEN')
    if token:
        print("âœ… å¾ç’°å¢ƒè®Šæ•¸ç²å–åˆ° Astra DB Token")
        return token
    
    # æ–¹æ³•2: å¾é…ç½®æ–‡ä»¶ç²å–
    config_file = Path("config/astra-token.txt")
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                token = f.read().strip()
            if token:
                print("âœ… å¾é…ç½®æ–‡ä»¶ç²å–åˆ° Astra DB Token")
                return token
        except Exception as e:
            print(f"âš ï¸  è®€å–é…ç½®æ–‡ä»¶å¤±æ•—: {e}")
    
    # æ–¹æ³•3: æç¤ºç”¨æˆ¶è¼¸å…¥
    print("ğŸ” è«‹è¼¸å…¥æ‚¨çš„ Astra DB API Token:")
    print("ğŸ’¡ æç¤º: æ‚¨ä¹Ÿå¯ä»¥å°‡ Token ä¿å­˜åˆ° config/astra-token.txt æ–‡ä»¶ä¸­")
    token = input("Token: ").strip()
    
    if token:
        # ä¿å­˜åˆ°é…ç½®æ–‡ä»¶ä¾›ä¸‹æ¬¡ä½¿ç”¨
        try:
            config_file.parent.mkdir(exist_ok=True)
            with open(config_file, 'w', encoding='utf-8') as f:
                f.write(token)
            print("âœ… Token å·²ä¿å­˜åˆ°é…ç½®æ–‡ä»¶")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ Token å¤±æ•—: {e}")
    
    return token

async def setup_astra_database():
    """è¨­ç½® Astra DB æ•¸æ“šåº«"""
    print("ğŸš€ è¨­ç½® Astra DB æ•¸æ“šåº«")
    print("=" * 50)
    print(f"ğŸ“Š æ•¸æ“šåº« ID: {ASTRA_DB_ID}")
    
    # ç²å– Token
    token = get_astra_token()
    if not token:
        print("âŒ æœªæä¾› Astra DB Token")
        return False
    
    try:
        # é€£æ¥åˆ° Astra DB
        print("ğŸ“¡ é€£æ¥åˆ° Astra DB...")
        client = DataAPIClient(token)
        database = client.get_database(ASTRA_DB_ID)
        
        # æ¸¬è©¦é€£æ¥
        print("âœ… æˆåŠŸé€£æ¥åˆ° Astra DB!")
        
        # ç²å–æ•¸æ“šåº«ä¿¡æ¯
        print("\nğŸ“‹ æ•¸æ“šåº«ä¿¡æ¯:")
        info = await database.info()
        print(f"   - åç¨±: {info.get('name', 'N/A')}")
        print(f"   - å€åŸŸ: {info.get('region', 'N/A')}")
        print(f"   - ç‹€æ…‹: {info.get('status', 'N/A')}")
        
        # å‰µå»ºé›†åˆ
        collection_name = "langflow_documents"
        print(f"\nğŸ“¦ å‰µå»ºé›†åˆ: {collection_name}")
        
        try:
            collection = await database.create_collection(
                collection_name,
                dimension=1536,  # OpenAI åµŒå…¥ç¶­åº¦
                metric="cosine"
            )
            print(f"âœ… é›†åˆ '{collection_name}' å‰µå»ºæˆåŠŸ")
        except Exception as e:
            if "already exists" in str(e).lower():
                print(f"â„¹ï¸  é›†åˆ '{collection_name}' å·²å­˜åœ¨")
                collection = database.get_collection(collection_name)
            else:
                raise e
        
        # è¨­ç½® OpenAI å®¢æˆ¶ç«¯
        print("\nğŸ¤– è¨­ç½® OpenAI å®¢æˆ¶ç«¯...")
        openai_client = OpenAI(api_key=CHATGPT_API_KEY)
        
        # ç¯„ä¾‹æ–‡æª”
        sample_docs = [
            {
                "text": "Langflow æ˜¯ä¸€å€‹é–‹æºçš„ AI æµç¨‹è‡ªå‹•åŒ–å¹³å°ï¼Œå¯ä»¥å¹«åŠ©ç”¨æˆ¶å‰µå»ºè¤‡é›œçš„ AI å·¥ä½œæµç¨‹ã€‚",
                "metadata": {"source": "langflow_docs", "category": "introduction"},
                "timestamp": "2024-01-01T00:00:00Z"
            },
            {
                "text": "Astra DB æ˜¯ DataStax æä¾›çš„å‘é‡æ•¸æ“šåº«æœå‹™ï¼Œå°ˆç‚º AI æ‡‰ç”¨è¨­è¨ˆï¼Œæ”¯æ´é«˜æ•ˆçš„å‘é‡æœç´¢ã€‚",
                "metadata": {"source": "astra_docs", "category": "database"},
                "timestamp": "2024-01-01T00:00:00Z"
            },
            {
                "text": "RAG (Retrieval-Augmented Generation) æ˜¯ä¸€ç¨®çµåˆæª¢ç´¢å’Œç”Ÿæˆçš„ AI æŠ€è¡“ï¼Œå¯ä»¥æé«˜å›ç­”çš„æº–ç¢ºæ€§ã€‚",
                "metadata": {"source": "ai_research", "category": "technique"},
                "timestamp": "2024-01-01T00:00:00Z"
            }
        ]
        
        # ç‚ºæ–‡æª”ç”ŸæˆåµŒå…¥å‘é‡
        print("ğŸ”„ ç”ŸæˆåµŒå…¥å‘é‡...")
        for doc in sample_docs:
            response = openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=doc["text"]
            )
            doc["$vector"] = response.data[0].embedding
        
        # æ’å…¥æ–‡æª”
        print("ğŸ“ æ’å…¥æ–‡æª”åˆ° Astra DB...")
        result = await collection.insert_many(sample_docs)
        print(f"âœ… æˆåŠŸæ’å…¥ {len(sample_docs)} å€‹æ–‡æª”")
        
        # æ¸¬è©¦æœç´¢
        print("\nğŸ” æ¸¬è©¦å‘é‡æœç´¢...")
        test_query = "ä»€éº¼æ˜¯ Langflowï¼Ÿ"
        query_response = openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=test_query
        )
        query_vector = query_response.data[0].embedding
        
        search_results = await collection.vector_find(
            query_vector,
            limit=2,
            fields=["text", "metadata"]
        )
        
        print(f"æŸ¥è©¢: {test_query}")
        print("æœç´¢çµæœ:")
        for i, result in enumerate(search_results, 1):
            print(f"{i}. {result.get('text', 'N/A')}")
            print(f"   å…ƒæ•¸æ“š: {result.get('metadata', {})}")
            print()
        
        print("ğŸ‰ Astra DB è¨­ç½®å®Œæˆï¼")
        print("ç¾åœ¨æ‚¨å¯ä»¥åœ¨ Langflow ä¸­ä½¿ç”¨é€™å€‹å‘é‡æ•¸æ“šåº«äº†ã€‚")
        
        return True
        
    except Exception as e:
        print(f"âŒ è¨­ç½®å¤±æ•—: {e}")
        print("\nğŸ’¡ å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ:")
        print("1. æª¢æŸ¥ API Token æ˜¯å¦æ­£ç¢º")
        print("2. ç¢ºèªæ•¸æ“šåº« ID æ˜¯å¦æ­£ç¢º")
        print("3. æª¢æŸ¥ç¶²è·¯é€£æ¥")
        return False

def create_env_file():
    """å‰µå»ºç’°å¢ƒè®Šæ•¸æ–‡ä»¶"""
    env_content = f"""# Astra DB é…ç½®
ASTRA_DB_ID={ASTRA_DB_ID}
ASTRA_DB_TOKEN=your_token_here

# OpenAI é…ç½®
OPENAI_API_KEY={CHATGPT_API_KEY}

# ä½¿ç”¨èªªæ˜:
# 1. å°‡ your_token_here æ›¿æ›ç‚ºæ‚¨çš„å¯¦éš› Astra DB Token
# 2. åœ¨ PowerShell ä¸­é‹è¡Œ: . .env
# 3. æˆ–è€…ç›´æ¥é‹è¡Œæ­¤è…³æœ¬ï¼Œå®ƒæœƒè‡ªå‹•è™•ç† Token
"""
    
    with open(".env", "w", encoding="utf-8") as f:
        f.write(env_content)
    
    print("ğŸ“ å·²å‰µå»º .env æ–‡ä»¶")
    print("ğŸ’¡ æ‚¨å¯ä»¥ç·¨è¼¯ .env æ–‡ä»¶ä¾†è¨­ç½®æ‚¨çš„ Astra DB Token")

async def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸ” å®‰å…¨çš„ Astra DB è¨­ç½®")
    print("=" * 50)
    
    # å‰µå»ºç’°å¢ƒè®Šæ•¸æ–‡ä»¶
    create_env_file()
    
    # è¨­ç½®æ•¸æ“šåº«
    success = await setup_astra_database()
    
    if success:
        print("\nâœ… è¨­ç½®å®Œæˆï¼")
        print("ğŸ“‹ ä¸‹ä¸€æ­¥:")
        print("1. åœ¨ Langflow ä¸­å°å…¥: examples/enhanced-astra-rag-flow.json")
        print("2. é–‹å§‹ä½¿ç”¨æ‚¨çš„ Astra DB å‘é‡æ•¸æ“šåº«")
    else:
        print("\nâŒ è¨­ç½®å¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤ä¿¡æ¯")

if __name__ == "__main__":
    asyncio.run(main())
